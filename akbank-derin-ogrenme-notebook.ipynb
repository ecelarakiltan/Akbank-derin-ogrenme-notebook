{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3027062,"sourceType":"datasetVersion","datasetId":1854018}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\nfrom sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n\n# GPU kontrolü\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Kullanılan cihaz:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T14:35:12.710713Z","iopub.execute_input":"2025-09-26T14:35:12.710982Z","iopub.status.idle":"2025-09-26T14:35:12.716547Z","shell.execute_reply.started":"2025-09-26T14:35:12.710962Z","shell.execute_reply":"2025-09-26T14:35:12.715868Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Kod Açıklaması\n\nPyTorch ve bazı yardımcı kütüphaneler kullanarak bir derin öğrenme projesinde temel hazırlıkları yapılmaktadır. \n\n# Kütüphaneler\n\n- `os`: Dosya ve klasör işlemleri.\n- `matplotlib.pyplot`: Veri görselleştirme.\n- `numpy`: Sayısal hesaplamalar.\n- `torch`, `torch.nn`, `torch.optim`: PyTorch ile derin öğrenme işlemleri.\n- `torch.utils.data`: Veri yükleme ve bölme.\n- `torchvision.datasets` ve `transforms`: Görüntü veri setleri ve ön işleme.\n- `sklearn.metrics`: Model performans ölçümleri.\n\n# GPU / CPU Kontrolü\n\n- `torch.cuda.is_available()`: CUDA destekli GPU var mı kontrol eder.\n- `torch.device(...)`: GPU varsa \"cuda\", yoksa \"cpu\" kullanır.\n- Bu sayede model ve veriler uygun cihazda çalıştırılır, eğitim süresi kısalır.\n\n\n","metadata":{}},{"cell_type":"code","source":"import torch\nprint(\"CUDA kullanılabilir mi?:\", torch.cuda.is_available())\nprint(\"Kullanılan cihaz:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T14:36:18.947807Z","iopub.execute_input":"2025-09-26T14:36:18.948043Z","iopub.status.idle":"2025-09-26T14:36:18.961896Z","shell.execute_reply.started":"2025-09-26T14:36:18.948022Z","shell.execute_reply":"2025-09-26T14:36:18.961182Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Kısa GPU Kontrolü\n\n- Sisteminizde GPU olup olmadığını hızlıca kontrol eder.\n- Model ve tensorlar uygun cihazda çalıştırılır, performans artar.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import Counter\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\n\n# --- Veri Yolu ---\ndata_dir = \"/kaggle/input/architectural-heritage-elements-image64-dataset\"\ntrain_dir = os.path.join(data_dir, \"train\")\ntest_dir = os.path.join(data_dir, \"test\")\n\n# --- Parametreler ---\nIMG_SIZE = 128\nBATCH_SIZE = 32\n\n# --- Transformlar ---\n# Temel transform (validation ve test için)\nbasic_transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n])\n\n# Augmentation (train için)\ntrain_transform_aug = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),        # Flip\n    transforms.RandomRotation(30),           # Rotation\n    transforms.ColorJitter(                  # Color Jitter\n        brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1\n    ),\n    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),  # Zoom\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n])\n\n# --- Dataset ---\nfull_train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform_aug)\n\n# Validation ayırma (%15)\nval_size = int(0.15 * len(full_train_dataset))\ntrain_size = len(full_train_dataset) - val_size\ntrain_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n\n# Validation dataset basic transform ile\nval_dataset.dataset.transform = basic_transform\n\n# Test dataset\ntest_dataset = datasets.ImageFolder(root=test_dir, transform=basic_transform)\n\n# --- DataLoader ---\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# --- Kontrol ---\nprint(\"Train:\", len(train_dataset), \"Validation:\", len(val_dataset), \"Test:\", len(test_dataset))\n\n# --- Sınıf isimleri ---\nclasses = full_train_dataset.classes\nprint(\"Sınıflar:\", classes)\n\n# --- Augmentation Örneklerini Görselleştirme ---\nimages, labels = next(iter(train_loader))\nplt.figure(figsize=(12, 6))\nfor i in range(6):\n    plt.subplot(2, 3, i+1)\n    img = images[i].permute(1, 2, 0).numpy()\n    img = img * 0.5 + 0.5  # normalize geri al\n    plt.imshow(img)\n    plt.title(classes[labels[i]])\n    plt.axis(\"off\")\nplt.suptitle(\"Data Augmentation Örnekleri\", fontsize=16)\nplt.show()\n\n# --- Veri seti istatistikleri ---\ntrain_labels = [label for _, label in full_train_dataset]\nlabel_counts = Counter(train_labels)\nprint(\"Train seti sınıf dağılımı:\")\nfor idx, count in label_counts.items():\n    print(f\"{classes[idx]}: {count} görsel\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T14:35:17.231649Z","iopub.execute_input":"2025-09-26T14:35:17.231912Z","iopub.status.idle":"2025-09-26T14:36:18.941058Z","shell.execute_reply.started":"2025-09-26T14:35:17.231893Z","shell.execute_reply":"2025-09-26T14:36:18.940426Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Veri Yükleme, Augmentation ve İstatistikler\n\nBu kod bloğu, Architectural Heritage veri setini PyTorch ile hazır hale getirir ve temel veri analizlerini yapar.\n\n1. **Veri Yolu ve Parametreler**\n   - `train_dir` ve `test_dir`: Eğitim ve test klasörlerinin yolu.\n   - `IMG_SIZE`: Görsellerin yeniden boyutlandırılacağı boyut (128x128).\n   - `BATCH_SIZE`: DataLoader ile verilerin batch hâlinde alınacağı büyüklük.\n\n2. **Transform ve Data Augmentation**\n   - `basic_transform`: Validation ve test verilerini tensor hâline getirip normalize eder.\n   - `train_transform_aug`: Eğitim verisine augmentation uygular:\n     - Horizontal flip, random rotation (30 derece),\n     - Color jitter (parlaklık, kontrast, renk doygunluğu ve hue),\n     - Random resized crop (zoom)\n   - Veri artırımı, modelin daha genelleştirilebilir olmasını sağlar.\n\n3. **Dataset ve DataLoader**\n   - `ImageFolder`: Görselleri klasör yapısına göre yükler.\n   - `random_split`: Eğitim setinden %15 validation ayırır.\n   - Validation ve test setleri `basic_transform` ile işlenir.\n   - `DataLoader`: Verileri batch hâlinde ve shuffle ile yükler.\n\n4. **Kontrol ve Sınıf İsimleri**\n   - Eğitim, validation ve test setlerinin boyutları ekrana yazdırılır.\n   - `classes`: Sınıf isimlerini listeler.\n\n5. **Augmentation Örneklerini Görselleştirme**\n   - Eğitim verisinden alınan örnek görseller gösterilir.\n   - Normalizasyon geri alınarak gerçek renkler görüntülenir.\n\n6. **Veri Seti İstatistikleri**\n   - Her sınıftaki görsel sayısı `Counter` ile hesaplanır.\n   - Sınıf dağılımı, veri dengesizliğini kontrol etmek için önemlidir.\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass CNNModel(nn.Module):\n    def __init__(self, num_classes, img_size=64, dropout_rate=0.5):\n        super(CNNModel, self).__init__()\n        \n        self.conv_block = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1),   # 64x64 → 32 filtre\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),               # 32x32\n\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),               # 16x16\n\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),               # 8x8\n\n            nn.Conv2d(128, 256, 3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),               # 4x4\n\n            nn.Conv2d(256, 512, 3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((2, 2))      # sabit boyut\n        )\n\n        # Dinamik flatten boyutu\n        dummy_input = torch.zeros(1, 3, img_size, img_size)\n        dummy_output = self.conv_block(dummy_input)\n        flatten_size = dummy_output.view(1, -1).size(1)\n\n        self.fc_block = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(flatten_size, 512),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(256, num_classes)   # Softmax yok → CrossEntropyLoss içinde var\n        )\n\n    def forward(self, x):\n        x = self.conv_block(x)\n        x = self.fc_block(x)\n        return x\n\n\n\nnum_classes = len(classes)\nmodel = CNNModel(num_classes).to(device)\nprint(model)\n\n# 4️⃣ Loss ve Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T14:36:38.495945Z","iopub.execute_input":"2025-09-26T14:36:38.496309Z","iopub.status.idle":"2025-09-26T14:36:38.821187Z","shell.execute_reply.started":"2025-09-26T14:36:38.496284Z","shell.execute_reply":"2025-09-26T14:36:38.820418Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN Modeli, Loss ve Optimizer\n\n## 1️⃣ Model Tanımı\n- `CNNModel` sınıfı, bir Convolutional Neural Network (CNN) tanımlar.\n- **Convolutional blokları:**\n  - Conv2d + BatchNorm + ReLU + MaxPool katmanları ile görsel özellikler çıkarılır.\n  - AdaptiveAvgPool2d ile son feature map boyutu sabitlenir.\n- **Fully Connected (FC) blokları:**\n  - Flatten sonrası Linear katmanlar ile sınıflandırma yapılır.\n  - ReLU aktivasyonu ve Dropout ile overfitting azaltılır.\n- `num_classes`: çıktı katmanındaki sınıf sayısı (CrossEntropyLoss ile uyumlu, softmax içermiyor).\n\n## 2️⃣ Dinamik Flatten Boyutu\n- `dummy_input` ile conv_block’tan geçen feature map boyutu hesaplanır.\n- Bu sayede farklı img_size parametrelerinde FC katmanları otomatik boyutlanır.\n\n## 3️⃣ Loss ve Optimizer\n- `criterion = nn.CrossEntropyLoss()`: Çok sınıflı sınıflandırma için uygun kayıp fonksiyonu.\n- `optimizer = optim.Adam(...)`: Model parametrelerini güncellemek için Adam optimizasyonu.\n- `scheduler = ReduceLROnPlateau(...)`: Validation kaybı durduğunda öğrenme oranını düşürür (faktor 0.5, patience 3).\n\n## 4️⃣ Cihaza Taşıma\n- `model.to(device)`: Model GPU (cuda) veya CPU üzerinde çalışacak şekilde ayarlanır.\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\n\n# --- Cihaz ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Kullanılan cihaz:\", device)\n\n# --- Hiperparametreler ---\nEPOCHS = 30\nBATCH_SIZE = 64  # GPU yeterliyse artır\nLEARNING_RATE = 0.001\nPATIENCE = 7  # Early stopping sabrı\nIMG_SIZE = 64  # Datasetin boyutu\nnum_classes = len(classes)\n\n# --- Data Augmentation ---\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8,1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n])\n\n# --- Dataset ve DataLoader (örn: ImageFolder kullanıyorsan) ---\ntrain_dataset.dataset.transform = train_transform\nval_dataset.dataset.transform = val_transform\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# --- Model ---\nclass CNNModel(nn.Module):\n    def __init__(self, num_classes, img_size=64):\n        super(CNNModel, self).__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2),\n            nn.Conv2d(32,64,3,padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2),\n            nn.Conv2d(64,128,3,padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2)\n        )\n\n        # Dinamik flatten boyutu için dummy forward\n        dummy_input = torch.zeros(1, 3, img_size, img_size)\n        dummy_output = self.conv_block(dummy_input)\n        flatten_size = dummy_output.view(1, -1).size(1)\n\n        self.fc_block = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(0.5),\n            nn.Linear(flatten_size, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, num_classes),\n            nn.Softmax(dim=1)\n        )\n\n    def forward(self, x):\n        x = self.conv_block(x)\n        x = self.fc_block(x)\n        return x\n\nmodel = CNNModel(num_classes).to(device)\n\n# --- Loss ve Optimizer ---\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n\n# --- Eğitim Döngüsü ---\nbest_val_loss = float('inf')\npatience_counter = 0\ntrain_losses, val_losses = [], []\ntrain_accs, val_accs = [], []\n\nfor epoch in range(EPOCHS):\n    # ---- TRAIN ----\n    model.train()\n    running_loss, running_corrects = 0.0, 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * images.size(0)\n        running_corrects += (outputs.argmax(1) == labels).sum().item()\n    \n    epoch_loss = running_loss / len(train_dataset)\n    epoch_acc = running_corrects / len(train_dataset)\n    train_losses.append(epoch_loss)\n    train_accs.append(epoch_acc)\n    \n    # ---- VALIDATION ----\n    model.eval()\n    running_loss, running_corrects = 0.0, 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * images.size(0)\n            running_corrects += (outputs.argmax(1) == labels).sum().item()\n    \n    val_loss = running_loss / len(val_dataset)\n    val_acc = running_corrects / len(val_dataset)\n    val_losses.append(val_loss)\n    val_accs.append(val_acc)\n    \n    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f} | Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n    \n    # ---- Early Stopping ve Checkpoint ----\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), \"best_model.pth\")\n    else:\n        patience_counter += 1\n        if patience_counter >= PATIENCE:\n            print(f\"Early stopping triggered at epoch {epoch+1}\")\n            break\n    \n    # ---- Scheduler ----\n    scheduler.step(val_loss)\n\n# --- Grafikler ---\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.plot(train_losses, label=\"Train Loss\")\nplt.plot(val_losses, label=\"Val Loss\")\nplt.legend()\nplt.title(\"Loss per Epoch\")\n\nplt.subplot(1,2,2)\nplt.plot(train_accs, label=\"Train Accuracy\")\nplt.plot(val_accs, label=\"Val Accuracy\")\nplt.legend()\nplt.title(\"Accuracy per Epoch\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T14:47:34.195780Z","iopub.execute_input":"2025-09-26T14:47:34.196511Z","iopub.status.idle":"2025-09-26T14:53:48.473615Z","shell.execute_reply.started":"2025-09-26T14:47:34.196486Z","shell.execute_reply":"2025-09-26T14:53:48.472834Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN Model Eğitimi ve Validasyon\n\n## 1️⃣ Cihaz ve Hiperparametreler\n- `device`: GPU varsa CUDA, yoksa CPU kullanılır.\n- `EPOCHS`, `BATCH_SIZE`, `LEARNING_RATE`: Eğitim döngüsü için temel hiperparametreler.\n- `PATIENCE`: Early stopping sabrı (validation kaybı iyileşmezse durdurma).\n\n## 2️⃣ Data Augmentation ve DataLoader\n- `train_transform`: Eğitim verisine augmentation uygular (crop, flip, rotation, color jitter).\n- `val_transform`: Validation verisi sadece normalize edilir.\n- `DataLoader`: Verileri batch hâlinde ve shuffle ile yükler.\n\n## 3️⃣ CNN Modeli\n- Convolutional bloklar: Conv2d + ReLU + MaxPool ile görsel özellik çıkarılır.\n- Fully Connected bloklar: Flatten → Linear → ReLU → Dropout → Linear → Softmax.\n- Dinamik flatten boyutu: Farklı `IMG_SIZE` değerleri ile uyumlu.\n\n## 4️⃣ Loss ve Optimizer\n- `criterion = nn.CrossEntropyLoss()`: Çok sınıflı sınıflandırma için uygun.\n- `optimizer = Adam`: Ağırlık güncellemesi için.\n- `scheduler = ReduceLROnPlateau`: Validation kaybı durursa öğrenme oranını düşürür.\n\n## 5️⃣ Eğitim Döngüsü\n- Her epoch için:\n  1. **Train**: Model `train()` modunda, loss ve doğruluk hesaplanır.\n  2. **Validation**: Model `eval()` modunda, gradient hesaplamadan loss ve doğruluk hesaplanır.\n  3. **Early Stopping ve Checkpoint**: Validation kaybı iyileşmezse eğitim durdurulur ve en iyi model kaydedilir.\n  4. **Scheduler**: Öğrenme oranı validation kaybına göre ayarlanır.\n\n## 6️⃣ Performans Görselleştirme\n- `train_losses` ve `val_losses` grafiği: Epoch başına kayıp değişimi.\n- `train_accs` ve `val_accs` grafiği: Epoch başına doğruluk değişimi.\n- Bu grafikler, modelin öğrenme sürecini ve overfitting durumunu analiz etmek için kullanılır.\n\n## 7️⃣ Eğitim Çıktısı Yorumları\n\n- Eğitim süreci **30 epoch** boyunca GPU üzerinde (`cuda`) tamamlandı.  \n- **Train Loss**: 2.10 → 1.64’e düştü → model eğitim setinde istikrarlı bir öğrenme gösterdi.  \n- **Validation Loss**: 2.01 → 1.69 civarına geriledi → doğrulama setinde de belirgin iyileşme var, ancak sonlarda küçük dalgalanmalar gözlendi.  \n- **Train Accuracy**: 0.34 → 0.82’ye yükseldi → eğitim setinde güçlü bir performans elde edildi.  \n- **Validation Accuracy**: 0.44 → 0.76’ya çıktı → doğrulama setinde de modelin genelleme kabiliyeti arttı.  \n- **Genel Değerlendirme**:\n  - Eğitim ve doğrulama kayıpları birbirine yakın seyretti, **overfitting düşük** seviyede.  \n  - Özellikle **10–20. epoch arası** modelin doğrulama performansında istikrarlı bir yükseliş görüldü.  \n  - En yüksek **validation accuracy ≈ %76.5 (Epoch 28)** değerine ulaşıldı.  \n  - Son epoch’larda train accuracy %82’ye çıktı ama validation accuracy biraz oynak kaldı → bu durum modelin artık sınır performansına yaklaşmaya başladığını gösteriyor.  \n  - Genel olarak model, hem eğitim hem de doğrulama verisinde **başarılı bir şekilde öğrenmiş ve dengeli genelleme yapabilmiş** durumda.  \n\n\n\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# --- Cihaz ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --- Test Transform ---\nIMG_SIZE = 64\ntest_transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n])\n\n# --- Test Dataset & Loader ---\ntest_dataset = datasets.ImageFolder(\"/kaggle/input/architectural-heritage-elements-image64-dataset/test\",\n                                    transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\nclasses = test_dataset.classes\n\n# --- Model yükleme ---\nmodel = CNNModel(num_classes=len(classes), img_size=IMG_SIZE).to(device)\nmodel.load_state_dict(torch.load(\"/kaggle/working/best_model.pth\", map_location=device))\nmodel.eval()\n\n# --- Confusion Matrix ve Classification Report ---\nall_preds, all_labels = [], []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        preds = outputs.argmax(1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Confusion Matrix\ncm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(10,8))\nsns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=classes, yticklabels=classes, cmap=\"Blues\")\nplt.xlabel(\"Tahmin\")\nplt.ylabel(\"Gerçek\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n# Classification Report\nprint(\"Classification Report:\\n\")\nprint(classification_report(all_labels, all_preds, target_names=classes, zero_division=0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T15:31:49.744840Z","iopub.execute_input":"2025-09-26T15:31:49.745075Z","iopub.status.idle":"2025-09-26T15:31:57.720994Z","shell.execute_reply.started":"2025-09-26T15:31:49.745059Z","shell.execute_reply":"2025-09-26T15:31:57.720318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Test Sonuçları\n\n## 1️⃣ Confusion Matrix\n- Confusion matrix, modelin hangi sınıfları doğru tahmin ettiğini ve hangi sınıflarda hata yaptığını görmemizi sağlar.  \n- Mavi tonlarda (Blues) görselleştirildiğinde, koyu hücreler daha yüksek doğru/yanlış sınıflandırma sayılarını temsil eder.  \n\n### Önemli Gözlemler:\n1. **Stained_glass**: Çok yüksek doğruluk → recall %95, precision %88. Model bu sınıfta en başarılı performansı sergiliyor.  \n2. **Gargoyle**: Yüksek başarı → recall %85, precision %78. Karışıklık düşük seviyede.  \n3. **Column ve Vault**: Güçlü f1-score (≈0.72–0.77). Model bu mimari öğeleri genellikle doğru ayırt edebiliyor.  \n4. **Dome(inner) ve Dome(outer)**: İyi performans (f1≈0.71–0.78), ancak inner–outer arasında karışıklık yaşanmış olabilir.  \n5. **Altar**: Precision yüksek (%81) ama recall düşük (%62) → model altar örneklerinin bir kısmını yanlış sınıflandırıyor.  \n6. **Bell_tower ve Apse**: Orta düzey başarı (f1≈0.51–0.57). Görsel benzerliklerden dolayı karışıklık yaşanıyor.  \n7. **Flying_buttress**: En zayıf sınıf → recall %24, f1=0.37. Model bu sınıfı tanımakta zorlanıyor, muhtemelen veri yetersizliği veya görsel karmaşıklık nedeniyle.  \n\n---\n\n## 2️⃣ Classification Report\n- **precision**: Belirli bir sınıf tahmini yapıldığında doğruluk oranı.  \n- **recall**: Gerçek sınıftaki örnekleri yakalama başarısı.  \n- **f1-score**: Precision ve recall’un dengeli ortalaması.  \n- **support**: Test setinde o sınıfa ait örnek sayısı.  \n\n### Önemli Gözlemler:\n- **En yüksek performans**: `stained_glass` (f1=0.91) → model bu sınıfta çok güçlü.  \n- **Yüksek performanslı diğer sınıflar**: `gargoyle` (0.81), `vault` (0.77), `dome(inner)` (0.78).  \n- **Orta performanslı sınıflar**: `altar` (0.70), `dome(outer)` (0.71), `column` (0.72).  \n- **Zayıf performanslı sınıflar**: `bell_tower` (0.57), `apse` (0.51), `flying_buttress` (0.37).  \n- **Genel accuracy**: %73 → Model test setindeki örneklerin yaklaşık 3/4’ünü doğru tahmin ediyor.  \n- **Macro avg f1-score = 0.69** → Sınıflar arası dengesizlik var, bazı sınıflar daha zayıf kalmış.  \n- **Weighted avg f1-score = 0.72** → Genel dağılıma göre dengeli performans kabul edilebilir seviyede.  \n\n---\n\n## 3️⃣ Genel Yorum\n- Model özellikle belirgin görsel özelliklere sahip sınıflarda (`stained_glass`, `gargoyle`, `vault`) güçlü performans sergiliyor.  \n- Daha zorlayıcı veya veri azlığı olan sınıflarda (`flying_buttress`, `apse`, `bell_tower`) hatalar artmış.  \n- **Altar** sınıfında precision yüksek ama recall düşük → model altar olmayan örnekleri altar olarak tahmin etmiyor (temkinli davranıyor), ama altar örneklerinin bir kısmını kaçırıyor.  \n- Performansı artırmak için:  \n  - Veri artırımı (özellikle `flying_buttress`, `apse`, `bell_tower` sınıflarında),  \n  - **Class-weighted loss** veya **focal loss** gibi yöntemler,  \n  - Daha derin/transfer learning tabanlı modeller (ör. ResNet, EfficientNet) kullanılabilir.  \n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n# --- Grad-CAM Fonksiyonu ---\ndef generate_gradcam(model, image, target_class):\n    model.eval()\n    image = image.unsqueeze(0).to(device)\n    gradients, activations = [], []\n\n    def forward_hook(module, input, output):\n        activations.append(output)\n    def backward_hook(module, grad_input, grad_output):\n        gradients.append(grad_output[0])\n\n    last_conv = model.conv_block[-3]\n    h1 = last_conv.register_forward_hook(forward_hook)\n    h2 = last_conv.register_full_backward_hook(backward_hook)\n\n    output = model(image)\n    model.zero_grad()\n    loss = output[0, target_class]\n    loss.backward()\n\n    grads = gradients[0][0].cpu().data.numpy()\n    acts = activations[0][0].cpu().data.numpy()\n    weights = np.mean(grads, axis=(1,2))\n    cam = np.zeros(acts.shape[1:], dtype=np.float32)\n    for i, w in enumerate(weights):\n        cam += w * acts[i]\n\n    cam = np.maximum(cam, 0)\n    cam = cv2.resize(cam, (IMG_SIZE, IMG_SIZE))\n    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n\n    h1.remove()\n    h2.remove()\n    return cam\n\n# --- Görselleştirme ---\nexamples_per_class = 2\nfig, axs = plt.subplots(len(classes), examples_per_class, figsize=(5*examples_per_class, 5*len(classes)))\n\nfor i, cls in enumerate(classes):\n    count = 0\n    for images, labels in test_loader:\n        for j in range(images.size(0)):\n            if classes[labels[j]] == cls:\n                image = images[j]\n                label = labels[j]\n                cam = generate_gradcam(model, image, label.item())\n                img = image.permute(1,2,0).cpu().numpy()\n                img = img * 0.5 + 0.5\n\n                ax = axs[i, count] if examples_per_class > 1 else axs[i]\n                ax.imshow(img)\n                ax.imshow(cam, cmap='jet', alpha=0.4)\n                ax.set_title(f\"Label: {cls}\")\n                ax.axis(\"off\")\n\n                count += 1\n                if count >= examples_per_class:\n                    break\n        if count >= examples_per_class:\n            break\n\nplt.suptitle(\"Grad-CAM Heatmap - Her Sınıftan Örnekler\", fontsize=16)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T15:34:16.045482Z","iopub.execute_input":"2025-09-26T15:34:16.045977Z","iopub.status.idle":"2025-09-26T15:34:24.859313Z","shell.execute_reply.started":"2025-09-26T15:34:16.045956Z","shell.execute_reply":"2025-09-26T15:34:24.858620Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Grad-CAM ile Görselleştirme\n\n## 1️⃣ Amaç\n- Grad-CAM (Gradient-weighted Class Activation Mapping), bir CNN modelinin hangi görsel bölgelerine odaklanarak tahmin yaptığını gösterir.\n- Bu sayede modelin karar mekanizmasını görselleştirebilir ve açıklanabilirliği artırabiliriz.\n\n## 2️⃣ İşleyiş\n1. **Forward Hook**: Son convolution katmanının aktivasyonlarını kaydeder.\n2. **Backward Hook**: Hedef sınıf için geri yayılım sırasında gradienleri kaydeder.\n3. **Ağırlıklı Aktivasyon**: Gradienlerin ortalaması alınarak her feature map için ağırlık hesaplanır.\n4. **Heatmap Oluşturma**: Feature map’ler ağırlıklarla çarpılır ve toplanır, ardından normalize edilip orijinal görüntü üzerine bindirilir.\n\n## 3️⃣ Kod Detayları\n- `generate_gradcam(model, image, target_class)`: Bir görüntü ve hedef sınıf için Grad-CAM haritası üretir.\n- `last_conv = model.conv_block[-3]`: Son convolution katmanı seçilir.\n- `cam = cv2.resize(cam, (IMG_SIZE, IMG_SIZE))`: Heatmap, orijinal görüntü boyutuna ölçeklenir.\n- `ax.imshow(cam, cmap='jet', alpha=0.4)`: Heatmap, orijinal görüntü üzerine yarı saydam olarak bindirilir.\n\n## 4️⃣ Görselleştirme\n- Her sınıftan `examples_per_class` adet örnek seçilir.\n- Görüntü ve Grad-CAM heatmap’i birlikte gösterilir.\n- Kırmızı bölgeler modelin tahmin için en çok odaklandığı alanları temsil eder.\n\n## 5️⃣ Yorum\n- Grad-CAM, modelin hangi bölgelere dikkat ettiğini göstererek yanlış tahminleri analiz etmemize yardımcı olur.\n- Örneğin, `stained_glass` veya `gargoyle` sınıflarında model doğru tahmin için karakteristik bölgeleri kullanıyor.\n- Zayıf sınıflarda (`flying_buttress`) heatmap daha dağınık olabilir, bu da modelin kararsız olduğunu gösterir.\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n\ndropout_values = [0.3, 0.5, 0.7]\nfixed_lr = 0.001\n\nbest_val_acc_dropout = 0\nbest_dropout = 0\n\n# Train/Val loss ve accuracy kayıtları\nhistory = {}\n\nfor dropout in dropout_values:\n    print(f\"Deneme: Dropout={dropout}, Learning Rate={fixed_lr}\")\n\n    model = CNNModel(num_classes=num_classes, img_size=64)\n    # FC bloktaki dropoutları değiştir\n    model.fc_block[1] = nn.Dropout(dropout)\n    model.fc_block[4] = nn.Dropout(dropout)\n    model = model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=fixed_lr)\n\n    # Küçük test epoch sayısı\n    EPOCHS_TEST = 5\n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n\n    for epoch in range(EPOCHS_TEST):\n        # ----- TRAIN -----\n        model.train()\n        running_loss, running_corrects, total = 0.0, 0, 0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * labels.size(0)\n            running_corrects += (outputs.argmax(1) == labels).sum().item()\n            total += labels.size(0)\n\n        epoch_loss = running_loss / total\n        epoch_acc = running_corrects / total\n        train_losses.append(epoch_loss)\n        train_accs.append(epoch_acc)\n\n        # ----- VALIDATION -----\n        model.eval()\n        running_loss, running_corrects, total = 0.0, 0, 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                running_loss += loss.item() * labels.size(0)\n                running_corrects += (outputs.argmax(1) == labels).sum().item()\n                total += labels.size(0)\n\n        val_epoch_loss = running_loss / total\n        val_epoch_acc = running_corrects / total\n        val_losses.append(val_epoch_loss)\n        val_accs.append(val_epoch_acc)\n\n    # Kayıtları history içine ekle\n    history[dropout] = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'train_accs': train_accs,\n        'val_accs': val_accs\n    }\n\n    final_val_acc = val_accs[-1]\n    print(f\"Validation Accuracy: {final_val_acc:.4f}\\n\")\n\n    if final_val_acc > best_val_acc_dropout:\n        best_val_acc_dropout = final_val_acc\n        best_dropout = dropout\n\nprint(\"En iyi Dropout:\", best_dropout, \"En iyi doğruluk:\", best_val_acc_dropout)\n\n# --- GRAFİKLER ---\nfor dropout in dropout_values:\n    plt.figure(figsize=(12,4))\n    \n    # Loss grafiği\n    plt.subplot(1,2,1)\n    plt.plot(history[dropout]['train_losses'], label='Train Loss')\n    plt.plot(history[dropout]['val_losses'], label='Val Loss')\n    plt.title(f\"Dropout={dropout} - Loss per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n    \n    # Accuracy grafiği\n    plt.subplot(1,2,2)\n    plt.plot(history[dropout]['train_accs'], label='Train Acc')\n    plt.plot(history[dropout]['val_accs'], label='Val Acc')\n    plt.title(f\"Dropout={dropout} - Accuracy per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.grid(True)\n    \n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T15:35:29.994381Z","iopub.execute_input":"2025-09-26T15:35:29.994967Z","iopub.status.idle":"2025-09-26T15:38:39.891285Z","shell.execute_reply.started":"2025-09-26T15:35:29.994946Z","shell.execute_reply":"2025-09-26T15:38:39.890484Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dropout Hiperparametre Denemeleri\n\n## 1️⃣ Amaç\n- Dropout, modelin overfitting (aşırı öğrenme) yapmasını önlemek için kullanılan bir regularizasyon yöntemidir.\n- Burada farklı dropout oranları (0.3, 0.5, 0.7) test edilerek en iyi doğruluk sağlayan oranı bulmayı amaçlıyoruz.\n\n## 2️⃣ İşleyiş\n1. **Model Yeniden Başlatma**: Her dropout değeri için CNN modeli sıfırlanıyor.\n2. **Dropout Ayarı**: Fully Connected bloktaki dropout değerleri güncelleniyor (`fc_block[1]` ve `fc_block[4]`).\n3. **Kısa Eğitim Döngüsü**: Her dropout için 5 epoch boyunca eğitim ve doğrulama yapılır.\n4. **Kayıt Tutma**: Train/Validation loss ve accuracy her epoch sonunda kaydedilir.\n5. **En İyi Dropout**: Son epoch’taki validation accuracy’ye göre en iyi dropout seçilir.\n\n## 3️⃣ Gözlemler\n- Grafikleri inceleyerek:\n  - **Düşük dropout** → overfitting riski artabilir (train loss çok düşük, val loss yüksek olabilir).\n  - **Yüksek dropout** → model underfitting yapabilir (train ve val accuracy düşük kalabilir).\n- Optimum dropout, validation doğruluğunu maksimize eden değerdir.\n\n## 4️⃣ Yorum\n- Kod çıktısına göre en iyi doğruluk sağlayan dropout değeri seçiliyor ve grafiklerle hem loss hem de accuracy trendleri görselleştiriliyor.\n- Bu deneme, model performansını iyileştirmek ve daha dengeli bir eğitim sağlamak için kritik bir adımdır.\n\n# Dropout Denemeleri Çıktı Yorumu\n\n# Dropout Denemeleri Sonuç Raporu\n\n## 1️⃣ Deneme Sonuçları\n- **Dropout = 0.3 → Validation Accuracy: 0.549**\n- **Dropout = 0.5 → Validation Accuracy: 0.574 ✅ (En iyi)**\n- **Dropout = 0.7 → Validation Accuracy: 0.549**\n\n---\n\n## 2️⃣ Yorum\n- **En iyi Dropout = 0.5**, çünkü en yüksek doğruluk (%57.4) bu değerde elde edildi.  \n- Düşük dropout (0.3), overfitting’i yeterince engelleyememiş.  \n- Yüksek dropout (0.7), model kapasitesini fazla kısıtladığı için underfitting’e sebep olmuş.  \n\n---\n\n## 3️⃣ Sonuç\n- Orta seviye dropout (0.5) bu veri setinde en uygun değer olarak görünüyor.  \n- Daha iyi sonuç için farklı **learning rate + dropout** kombinasyonlarının birlikte test edilmesi önerilir.  \n\n","metadata":{}},{"cell_type":"code","source":"learning_rates = [0.0001, 0.001, 0.01]\nfixed_dropout = 0.5  # Dropout en iyi değerde sabit\n\nbest_val_acc_lr = 0\nbest_lr = 0\nhistory_lr = {}\n\nfor lr in learning_rates:\n    print(f\"Deneme: Dropout={fixed_dropout}, Learning Rate={lr}\")\n\n    model = CNNModel(num_classes=num_classes, img_size=64)\n    model.fc_block[1] = nn.Dropout(fixed_dropout)\n    model.fc_block[4] = nn.Dropout(fixed_dropout)\n    model = model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    EPOCHS_TEST = 5\n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n\n    for epoch in range(EPOCHS_TEST):\n        # ----- TRAIN -----\n        model.train()\n        running_loss, running_corrects, total = 0.0, 0, 0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * labels.size(0)\n            running_corrects += (outputs.argmax(1) == labels).sum().item()\n            total += labels.size(0)\n\n        epoch_loss = running_loss / total\n        epoch_acc = running_corrects / total\n        train_losses.append(epoch_loss)\n        train_accs.append(epoch_acc)\n\n        # ----- VALIDATION -----\n        model.eval()\n        running_loss, running_corrects, total = 0.0, 0, 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                running_loss += loss.item() * labels.size(0)\n                running_corrects += (outputs.argmax(1) == labels).sum().item()\n                total += labels.size(0)\n\n        val_epoch_loss = running_loss / total\n        val_epoch_acc = running_corrects / total\n        val_losses.append(val_epoch_loss)\n        val_accs.append(val_epoch_acc)\n\n    # Kayıtları history içine ekle\n    history_lr[lr] = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'train_accs': train_accs,\n        'val_accs': val_accs\n    }\n\n    final_val_acc = val_accs[-1]\n    print(f\"Validation Accuracy: {final_val_acc:.4f}\\n\")\n\n    if final_val_acc > best_val_acc_lr:\n        best_val_acc_lr = final_val_acc\n        best_lr = lr\n\nprint(\"En iyi Learning Rate:\", best_lr, \"En iyi doğruluk:\", best_val_acc_lr)\n\n# --- GRAFİKLER ---\nfor lr in learning_rates:\n    plt.figure(figsize=(12,4))\n    \n    # Loss grafiği\n    plt.subplot(1,2,1)\n    plt.plot(history_lr[lr]['train_losses'], label='Train Loss')\n    plt.plot(history_lr[lr]['val_losses'], label='Val Loss')\n    plt.title(f\"Learning Rate={lr} - Loss per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n    \n    # Accuracy grafiği\n    plt.subplot(1,2,2)\n    plt.plot(history_lr[lr]['train_accs'], label='Train Acc')\n    plt.plot(history_lr[lr]['val_accs'], label='Val Acc')\n    plt.title(f\"Learning Rate={lr} - Accuracy per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.grid(True)\n    \n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T15:56:23.150484Z","iopub.execute_input":"2025-09-26T15:56:23.151054Z","iopub.status.idle":"2025-09-26T15:59:29.669875Z","shell.execute_reply.started":"2025-09-26T15:56:23.151032Z","shell.execute_reply":"2025-09-26T15:59:29.669195Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Learning Rate Denemeleri Sonuç Raporu\n\n## 1️⃣ Deneme Sonuçları\n- **Learning Rate = 0.0001 → Validation Accuracy: 0.5076**\n  - Çok düşük → model yavaş öğreniyor, doğruluk sınırlı kalıyor.  \n- **Learning Rate = 0.001 → Validation Accuracy: 0.5734 ✅**\n  - Orta seviye → model dengeli şekilde öğreniyor, en iyi sonuç burada elde edildi.  \n- **Learning Rate = 0.01 → Validation Accuracy: 0.1606 ⚠️**\n  - Çok yüksek → model dengesiz öğreniyor (overshooting), doğruluk ciddi şekilde düşüyor.  \n\n---\n\n## 2️⃣ Yorum\n- **En iyi Learning Rate = 0.001**  \n- Çok düşük learning rate eğitim süresini uzatıyor.  \n- Çok yüksek learning rate ise modelin öğrenmesini bozuyor.  \n\n---\n\n## 3️⃣ Sonuç\n- **Learning Rate = 0.001**, bu veri seti ve model için optimum değer olarak belirlendi.  \n- Sonraki tam eğitimlerde **Dropout = 0.5** ve **Learning Rate = 0.001** birlikte kullanılabilir.  \n","metadata":{}},{"cell_type":"code","source":"dropout_values = [0.3, 0.5, 0.7]\nlearning_rates = [0.0001, 0.001, 0.01]\n\nEPOCHS_TEST = 5\nhistory_comb = {}\n\nfor dropout in dropout_values:\n    for lr in learning_rates:\n        key = f\"Dropout={dropout}_LR={lr}\"\n        print(f\"Deneme: {key}\")\n\n        model = CNNModel(num_classes=num_classes, img_size=64)\n        model.fc_block[1] = nn.Dropout(dropout)\n        model.fc_block[4] = nn.Dropout(dropout)\n        model = model.to(device)\n\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n        train_losses, val_losses = [], []\n        train_accs, val_accs = [], []\n\n        for epoch in range(EPOCHS_TEST):\n            # TRAIN\n            model.train()\n            running_loss, running_corrects, total = 0.0, 0, 0\n            for images, labels in train_loader:\n                images, labels = images.to(device), labels.to(device)\n                optimizer.zero_grad()\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                running_loss += loss.item() * labels.size(0)\n                running_corrects += (outputs.argmax(1) == labels).sum().item()\n                total += labels.size(0)\n            train_losses.append(running_loss/total)\n            train_accs.append(running_corrects/total)\n\n            # VALIDATION\n            model.eval()\n            running_loss, running_corrects, total = 0.0, 0, 0\n            with torch.no_grad():\n                for images, labels in val_loader:\n                    images, labels = images.to(device), labels.to(device)\n                    outputs = model(images)\n                    loss = criterion(outputs, labels)\n                    running_loss += loss.item() * labels.size(0)\n                    running_corrects += (outputs.argmax(1) == labels).sum().item()\n                    total += labels.size(0)\n            val_losses.append(running_loss/total)\n            val_accs.append(running_corrects/total)\n\n        history_comb[key] = {\n            'train_losses': train_losses,\n            'val_losses': val_losses,\n            'train_accs': train_accs,\n            'val_accs': val_accs\n        }\n\n# --- GRAFİKLER ---\nfor key, values in history_comb.items():\n    plt.figure(figsize=(12,4))\n    \n    # Loss grafiği\n    plt.subplot(1,2,1)\n    plt.plot(values['train_losses'], label='Train Loss')\n    plt.plot(values['val_losses'], label='Val Loss')\n    plt.title(f\"{key} - Loss per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n    \n    # Accuracy grafiği\n    plt.subplot(1,2,2)\n    plt.plot(values['train_accs'], label='Train Acc')\n    plt.plot(values['val_accs'], label='Val Acc')\n    plt.title(f\"{key} - Accuracy per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.grid(True)\n    \n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:01:17.982420Z","iopub.execute_input":"2025-09-26T16:01:17.983002Z","iopub.status.idle":"2025-09-26T16:10:23.019396Z","shell.execute_reply.started":"2025-09-26T16:01:17.982980Z","shell.execute_reply":"2025-09-26T16:10:23.018627Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hiperparametre Kombinasyonları (Dropout ve Learning Rate) Denemesi\n\n## Yapılan İşlemler\n- **Dropout değerleri:** `[0.3, 0.5, 0.7]`  \n- **Learning Rate değerleri:** `[0.0001, 0.001, 0.01]`  \n- Her kombinasyon için:\n  1. CNN modeli oluşturuldu.\n  2. Fully-connected bloktaki dropout değerleri güncellendi.\n  3. Model belirtilen learning rate ile Adam optimizer kullanılarak eğitildi.\n  4. 5 epoch boyunca eğitim ve doğrulama kayıpları ile doğrulukları kaydedildi.\n  5. Sonuçlar `history_comb` dictionary’sine kaydedildi.\n\n## Grafikler\n- Her kombinasyon için **Loss ve Accuracy** grafikleri çizildi.\n- Loss grafikleri: Modelin eğitim ve doğrulama sırasında kaybının nasıl değiştiğini gösterir.\n- Accuracy grafikleri: Modelin doğrulama performansını ve overfitting/underfitting eğilimlerini gösterir.\n\n## Yorum\n- **Amaç:** Hem dropout hem learning rate değerlerini birlikte deneyerek en iyi kombinasyonu belirlemek.\n- Grafikler üzerinden hangi kombinasyonun hem düşük loss hem yüksek doğruluk verdiği görselleştirilebilir.\n- Bu yöntem, hiperparametre optimizasyonunun basit bir görselleştirme ve gözlem yoludur.\n\n---\n\n# Hiperparametre Tarama Sonuçları\n\n## Denenen Kombinasyonlar\n- Dropout değerleri: `0.3, 0.5, 0.7`\n- Learning Rate değerleri: `0.0001, 0.001, 0.01`\n- Kombinasyonlar:\n  1. Dropout=0.3, LR=0.0001 → Val Accuracy ≈ 0.485  \n  2. Dropout=0.3, LR=0.001  → Val Accuracy ≈ 0.604 ✅  \n  3. Dropout=0.3, LR=0.01   → Val Accuracy ≈ 0.117  \n  4. Dropout=0.5, LR=0.0001 → Val Accuracy ≈ 0.508  \n  5. Dropout=0.5, LR=0.001  → Val Accuracy ≈ 0.573  \n  6. Dropout=0.5, LR=0.01   → Val Accuracy ≈ 0.161  \n  7. Dropout=0.7, LR=0.0001 → Val Accuracy ≈ 0.579  \n  8. Dropout=0.7, LR=0.001  → Val Accuracy ≈ 0.549  \n  9. Dropout=0.7, LR=0.01   → Val Accuracy ≈ çok düşük  \n\n## Yorum\n- **En iyi kombinasyon:** `Dropout=0.3` ve `Learning Rate=0.001`  \n- Bu kombinasyon, validation setinde en yüksek doğruluk sağladı (~60.4%).  \n- Çok yüksek dropout (0.5, 0.7) veya çok yüksek LR (0.01) doğruluğu ciddi şekilde düşürdü.  \n- Düşük LR (0.0001) yavaş öğrenme sebebiyle performansı sınırladı.  \n- Bu sonuçlar, modelin **overfitting’i önlerken yeterince öğrenmesini sağlayacak dropout ve LR değerlerini** görsel olarak da destekliyor.  \n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom sklearn.metrics import accuracy_score\n\n# --- Cihaz ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --- Test Transform ---\nIMG_SIZE = 64\ntest_transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n])\n\n# --- Test Dataset & Loader ---\ntest_dataset = datasets.ImageFolder(\"/kaggle/input/architectural-heritage-elements-image64-dataset/test\",\n                                    transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# --- Model yükleme ---\nmodel = CNNModel(num_classes=len(test_dataset.classes), img_size=IMG_SIZE).to(device)\nmodel.load_state_dict(torch.load(\"/kaggle/working/best_model.pth\", map_location=device))\nmodel.eval()\n\n# --- Model Başarı Skoru ---\nall_preds, all_labels = [], []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        preds = outputs.argmax(dim=1)\n        all_preds.extend(preds.cpu().tolist())\n        all_labels.extend(labels.cpu().tolist())\n\nacc = accuracy_score(all_labels, all_preds)\nprint(f\"✅ Model Başarı Skoru (Accuracy): {acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:40.920979Z","iopub.execute_input":"2025-09-26T16:13:40.921311Z","iopub.status.idle":"2025-09-26T16:13:44.132873Z","shell.execute_reply.started":"2025-09-26T16:13:40.921282Z","shell.execute_reply":"2025-09-26T16:13:44.132069Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Başarı Skoru (Test Seti)\n\nBu kod, eğitilmiş CNN modelini test setinde değerlendirir ve doğruluk skorunu hesaplar:\n\n- Test verisi `64x64` boyutunda normalize edilip yüklenir.  \n- Model, `best_model.pth` ağırlıklarıyla yüklenir.  \n- Modelin tahminleri alınır ve gerçek etiketlerle karşılaştırılır.  \n- `sklearn.metrics.accuracy_score` kullanılarak doğruluk hesaplanır.\n\nBu sonuç, modelin test setinde yaklaşık **%72.8 doğruluk** sağladığını gösterir. Validation skorlarına yakın olması, modelin iyi genelleştiğini gösterir.\n\n\n\n","metadata":{}}]}